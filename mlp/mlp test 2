def plot_classification(X, Y, k, weights, bias, colors, num_features):
    if num_features == 3:
        # Tracé de la séparation des classes pour données 3D
        x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1
        y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1
        z_min, z_max = X[:, 2].min() - 0.1, X[:, 2].max() + 0.1
        step = 0.1

        xx, yy, zz = np.meshgrid(
            np.arange(x_min, x_max, step),
            np.arange(y_min, y_max, step),
            np.arange(z_min, z_max, step)
        )

        grid_points = np.c_[xx.ravel(), yy.ravel(), zz.ravel()]
        grid_predictions = np.dot(grid_points, weights) + bias  # Utiliser weights directement

        # Tracé des points d'entraînement avec des couleurs différentes pour chaque classe
        class_0 = X[Y[:, 0] < 0]
        class_1 = X[Y[:, 0] > 0]
        class_2 = X[np.argmax(Y, axis=1) == 2]

        fig = plt.figure()
        ax = fig.add_subplot(111, projection='3d')
        ax.scatter(class_0[:, 0], class_0[:, 1], class_0[:, 2], color='blue', edgecolor='k', label='Classe 0')
        ax.scatter(class_1[:, 0], class_1[:, 1], class_1[:, 2], color='red', edgecolor='k', label='Classe 1')
        ax.scatter(class_2[:, 0], class_2[:, 1], class_2[:, 2], color='green', edgecolor='k', label='Classe 2')

        # Calcul de la surface de séparation pour un modèle 3D
        for i in range(k):
            zz = -(weights[0, i] * xx + weights[1, i] * yy + bias[0, i]) / weights[2, i]
            ax.plot_surface(xx, yy, zz, alpha=0.5)

        plt.legend()
        plt.show()

    elif num_features == 2:
        # Tracé de la séparation des classes pour données 2D
        x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1
        y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1
        step = 0.01

        xx, yy = np.meshgrid(np.arange(x_min, x_max, step), np.arange(y_min, y_max, step))
        grid_points = np.c_[xx.ravel(), yy.ravel()]
        grid_predictions = np.dot(grid_points, weights) + bias  # Utiliser weights directement

        if k <= 2:
            # Tracé des points d'entraînement avec des couleurs différentes pour chaque classe
            class_0 = X[Y[:, 0] < 0]
            class_1 = X[Y[:, 0] > 0]

            plt.scatter(class_0[:, 0], class_0[:, 1], color='blue', edgecolor='k', label='Classe 0')
            plt.scatter(class_1[:, 0], class_1[:, 1], color='red', edgecolor='k', label='Classe 1')

            # Tracer la séparation des classes
            contour = grid_predictions[:, 0].reshape(xx.shape)
            plt.contourf(xx, yy, contour, levels=[-np.inf, 0, np.inf], colors=['blue', 'red'], alpha=0.5)

        else:
            # Tracé des points d'entraînement avec des couleurs différentes pour chaque classe
            class_0 = X[Y[:, 0] < 0]
            class_1 = X[Y[:, 0] > 0]
            class_2 = X[np.argmax(Y, axis=1) == 2]

            plt.scatter(class_0[:, 0], class_0[:, 1], color='red', edgecolor='k', label='Classe 0')
            plt.scatter(class_1[:, 0], class_1[:, 1], color='blue', edgecolor='k', label='Classe 1')
            plt.scatter(class_2[:, 0], class_2[:, 1], color='green', edgecolor='k', label='Classe 2')

            # Tracer la séparation des classes
            contour = np.argmax(grid_predictions, axis=1).reshape(xx.shape)
            plt.contourf(xx, yy, contour, levels=[-np.inf, 0.5, 1.5, np.inf], colors=['blue', 'red', 'green'], alpha=0.4)

        plt.legend()
        plt.show()



def test_train(X, Y, arr, alpha, nb_iter, is_classification):
    arr_ptr = (ctypes.c_longlong * len(arr))(*arr)
    model_ptr = lib.create_mlp_model(arr_ptr, len(arr))

    # Appeler la fonction train_mlp_model pour entraîner le modèle
    lib.train_mlp_model(model_ptr,
                        X.ctypes.data_as(ctypes.POINTER(ctypes.c_double)),  # données d'entrée
                        ctypes.c_int64(X.shape[0]),  # nombre de lignes dans les données d'entrée
                        ctypes.c_int64(X.shape[1]),  # nombre de colonnes dans les données d'entrée
                        Y.ctypes.data_as(ctypes.POINTER(ctypes.c_double)),  # données de sortie
                        ctypes.c_int64(Y.shape[1]),  # nombre de colonnes dans les données de sortie
                        ctypes.c_double(alpha),  # alpha
                        ctypes.c_int64(nb_iter),  # nb_iter
                        ctypes.c_bool(is_classification))  # is_classification

    output_ptr = lib.predict_mlp_model(model_ptr,
                                       X.ctypes.data_as(ctypes.POINTER(ctypes.c_double)),
                                       ctypes.c_int64(X.shape[0]),
                                       ctypes.c_int64(X.shape[1]),
                                       ctypes.c_bool(is_classification))

    # Obtenir le nombre total d'éléments que output_ptr est censé pointer
    num_elements = X.shape[0]

    # Créez un type de tableau ctypes de la bonne taille
    ArrayType = ctypes.c_double * num_elements

    # Cast le pointeur en un tableau ctypes
    output_array = ctypes.cast(output_ptr, ctypes.POINTER(ArrayType)).contents

    # Convertir le tableau ctypes en un numpy array
    output_np_array = np.ctypeslib.as_array(output_array)

    print(output_np_array)

    # Calculer les poids manuellement pour une ligne de séparation
    # On utilise la méthode des moindres carrés pour trouver une séparation linéaire
    A = np.c_[X, np.ones(X.shape[0])]
    w = np.linalg.pinv(A).dot(Y)

    # Reshape weights to have the correct dimensions
    weights = w[:-1].reshape(X.shape[1], -1)
    bias = w[-1].reshape(1, -1)

    colors = {1: 'red', 0: 'blue'}

    # Utiliser la nouvelle fonction plot_classification
    plot_classification(X, Y, len(Y[0]), weights, bias, colors, X.shape[1])

    # Libérer la mémoire du modèle
    lib.delete_mlp_model(model_ptr)